{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ie-IdoiHY4j_"
      },
      "source": [
        "![](nb_images/guiones_wave.jpeg \"A big day at Playa Guiones.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjICBZ9vY4kA"
      },
      "source": [
        "Well, you guessed it: it's time for us to learn PySpark!\n",
        "\n",
        "I know, I know, I can hear you screaming into your pillow. Indeed we just spent all that time converting from R and learning python and why the hell do we need yet another API for working with dataframes?\n",
        "\n",
        "That's a totally fair question.\n",
        "\n",
        "So what happens when we're working on something in the real world, where datasets get large in a hurry, and we suddenly have a dataframe that no longer fits into memory?\n",
        "We need a way for our computations and datasets to scale across multiple nodes in a distributed system without having to get too fussy about all the distributed compute details.\n",
        "\n",
        "Enter PySpark.\n",
        "\n",
        "I think it's fair to think of PySpark as a python package for working with arbitrarily large dataframes, i.e., it's like pandas but scalable.\n",
        "It's built on top of [Apache Spark](https://spark.apache.org/), a unified analytics engine for large-scale data processing.\n",
        "[PySpark](https://spark.apache.org/docs/latest/api/python/)  is essentially a way to access the functionality of spark via python code.\n",
        "While there are other high-level interfaces to Spark (such as Java, Scala, and R), for data scientists who are already working extensively with python, PySpark will be the natural interface of choice.\n",
        "PySpark also has great integration with [SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html), and it has a companion machine learning library called [MLlib](https://spark.apache.org/mllib/) that's more or less a scalable scikit-learn (maybe we can cover it in a future post).\n",
        "\n",
        "So, here's the plan.\n",
        "First we're going to get set up to run PySpark locally in a jupyter notebook on our laptop.\n",
        "This is my preferred environment for interactively playing with PySpark and learning the ropes.\n",
        "Then we're going to get up and running in PySpark as quickly as possible by reviewing the most essential functionality for working with dataframes and comparing it to how we would do things in pandas.\n",
        "Once we're comfortable running PySpark on the laptop, it's going to be much easier to jump onto a distributed cluster and run PySpark at scale.\n",
        "\n",
        "Let's do this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfTLzkEDY4kA"
      },
      "source": [
        "## How to Run PySpark in a Jupyter Notebook on Your Laptop\n",
        "\n",
        "Ok, I'm going to walk us through how to get things installed on a Mac or Linux machine where we're using homebrew and conda to manage virtual environments.\n",
        "If you have a different setup, your favorite search engine will help you get PySpark set up locally."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXPNx_oqY4kB"
      },
      "source": [
        "### Install Spark\n",
        "\n",
        "Most of the Spark sourcecode is written in Scala, so first we install Scala.\n",
        "\n",
        "```\n",
        "$ brew install scala\n",
        "```\n",
        "\n",
        "Install Spark.\n",
        "\n",
        "```\n",
        "$ brew install apache-spark\n",
        "```\n",
        "\n",
        "Check where Spark is installed.\n",
        "```\n",
        "$ brew info apache-spark\n",
        "apache-spark: stable 3.1.1, HEAD\n",
        "Engine for large-scale data processing\n",
        "https://spark.apache.org/\n",
        "/usr/local/Cellar/apache-spark/3.1.2 (1,361 files, 242.6MB) *\n",
        "...\n",
        "```\n",
        "\n",
        "Set the Spark home environment variable to the path returned by `brew info` with `/libexec` appended to the end.\n",
        "Don't forget to add the export to your `.zshrc` file too.\n",
        "\n",
        "```\n",
        "$ export SPARK_HOME=/usr/local/Cellar/apache-spark/3.1.2/libexec\n",
        "```\n",
        "\n",
        "Test the installation by starting the Spark shell.\n",
        "\n",
        "```\n",
        "$ spark-shell\n",
        "...\n",
        "Welcome to\n",
        "      ____              __\n",
        "     / __/__  ___ _____/ /__\n",
        "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
        "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.1.1\n",
        "      /_/\n",
        "         \n",
        "Using Scala version 2.12.10 (OpenJDK 64-Bit Server VM, Java 14.0.1)\n",
        "Type in expressions to have them evaluated.\n",
        "Type :help for more information.\n",
        "\n",
        "scala>\n",
        "```\n",
        "\n",
        "If you get the `scala>` prompt, then you've successfully installed Spark on your laptop!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-SqqwHsY4kB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YjFcchnY4kB"
      },
      "source": [
        "### Install PySpark\n",
        "\n",
        "Use conda to install the PySpark python package.\n",
        "As usual, it's advisable to do this in a new virtual environment.\n",
        "\n",
        "\n",
        "```\n",
        "$ conda install pyspark\n",
        "```\n",
        "\n",
        "You should be able to launch an interactive PySpark REPL by saying pyspark.\n",
        "\n",
        "```\n",
        "$ pyspark\n",
        "...\n",
        "Welcome to\n",
        "      ____              __\n",
        "     / __/__  ___ _____/ /__\n",
        "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
        "   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.1.2\n",
        "      /_/\n",
        "\n",
        "Using Python version 3.8.3 (default, Jul  2 2020 11:26:31)\n",
        "Spark context Web UI available at http://192.168.100.47:4041\n",
        "Spark context available as 'sc' (master = local[*], app id = local-1624127229929).\n",
        "SparkSession available as 'spark'.\n",
        ">>>\n",
        "```\n",
        "\n",
        "This time we get a familiar python `>>>` prompt.\n",
        "This is an interactive shell where we can easily experiment with PySpark.\n",
        "Feel free to run the example code in this post here in the PySpark shell, or, if you prefer a notebook, read on and we'll get set up to run PySpark in a jupyter notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFfvSb1YY4kC"
      },
      "source": [
        "### The Spark Session Object\n",
        "\n",
        "You may have noticed that when we launched that PySpark interactive shell, it told us that something called `SparkSession` was available as `'spark'`.\n",
        "So basically, what's happening here is that when we launch the pyspark shell, it instantiates an object called `spark` which is an instance of class `pyspark.sql.session.SparkSession`.\n",
        "The spark session object is going to be our entry point for all kinds of PySpark functionality, i.e., we're going to be saying things like `spark.this()` and `spark.that()` to make stuff happen.\n",
        "\n",
        "The PySpark interactive shell is kind enough to instantiate one of these spark session objects for us automatically.\n",
        "However, when we're using another interface to PySpark (like say a jupyter notebook running a python kernal), we'll have to make a spark session object for ourselves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZmFNjoAY4kC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S89LiP3MY4kC"
      },
      "source": [
        "### Create a PySpark Session in a Jupyter Notebook\n",
        "\n",
        "There are a few ways to run PySpark in jupyter which you can read about [here](https://www.datacamp.com/community/tutorials/apache-spark-python).\n",
        "\n",
        "For derping around with PySpark on your laptop, I think the best way is to instantiate a spark session from a  jupyter notebook running on a regular python kernel.\n",
        "The method we'll use involves running a standard jupyter notebook session with a python kernal and using the findspark package to initialize the spark session.\n",
        "So, first install the findspark package.\n",
        "\n",
        "```\n",
        "$ conda install findspark\n",
        "```\n",
        "\n",
        "Launch jupyter as usual.\n",
        "\n",
        "```\n",
        "$ jupyter notebook\n",
        "```\n",
        "\n",
        "\n",
        "Go ahead and fire up a new notebook using a regular python 3 kernal.\n",
        "Once you land inside the notebook, there are a couple things we need to do to get a spark session instantiated.\n",
        "You can think of this as boilerplate code that we need to run in the first cell of a notebook where we're going to use PySpark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhPe7rNuY4kC"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "import findspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "findspark.init()\n",
        "spark = SparkSession.builder.appName('My Spark App').getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_UYr9GMdY-Ud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21sDsonUY4kD"
      },
      "source": [
        "First we're running findspark's `init()` method to find our Spark installation. If you run into errors here,\n",
        "make sure you got the `SPARK_HOME` environment variable correctly set in the install instructions above.\n",
        "Then we instantiate a spark session as `spark`.\n",
        "Once you run this, you're ready to rock and roll with PySpark in your jupyter notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRd3OIOmY4kD"
      },
      "source": [
        "> Note: Spark provides a handy web UI that you can use for monitoring and debugging. Once you instantiate the spark session You can open the UI in your web browser at [http://localhost:4040/jobs/](http://localhost:4040/jobs/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NuWaKPKvY4kD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}